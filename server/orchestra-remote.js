/**
 * Orchestra Remote - HTTP-based AI client with automatic fallbacks
 * Order of operations:
 * 1. Try local BigDaddyG bridge (native Ollama models)
 * 2. Try remote HTTP API if API key provided
 * 3. Fall back to built-in templated responses
 */

class OrchestraRemote {
  constructor(apiBase, apiKey) {
    this.apiBase = apiBase || process.env.API_BASE || 'https://api.deepseek.com/v1';
    this.apiKey = apiKey || process.env.API_KEY || '';
  }

  async generate(prompt, model = 'deepseek-chat') {
    // 1. Prefer locally loaded models exposed by the Electron bridge
    try {
      const response = await fetch('http://127.0.0.1:11435/api/generate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ model, prompt }),
        signal: AbortSignal.timeout(5000)
      });

      if (response.ok) {
        const data = await response.json();
        console.log('[OrchestraRemote] ✅ Using BigDaddyGCore models via bridge');
        return data.response;
      }
    } catch (error) {
      console.log('[OrchestraRemote] Bridge unavailable, falling back to remote AI');
    }

    // 2. Use remote API if credentials provided
    if (this.apiKey) {
      try {
        const response = await fetch(`${this.apiBase}/chat/completions`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            Authorization: `Bearer ${this.apiKey}`
          },
          body: JSON.stringify({
            model,
            messages: [{ role: 'user', content: prompt }],
            stream: false
          }),
          signal: AbortSignal.timeout(30000)
        });

        if (response.ok) {
          const data = await response.json();
          const content = data?.choices?.[0]?.message?.content || data.response || '';
          console.log('[OrchestraRemote] ✅ Using configured remote AI API');
          return content;
        }
      } catch (error) {
        console.log('[OrchestraRemote] Remote API failed:', error.message);
      }
    }

    // 3. Final fallback - deterministic built-in response
    console.log('[OrchestraRemote] Using built-in fallback response');
    return this.generateBuiltIn(prompt, model);
  }

  generateBuiltIn(prompt, modelType) {
    const sanitizedPrompt = typeof prompt === 'string' ? prompt.trim() : '';
    if (modelType && modelType.includes('coder')) {
      return `// Generated for: ${sanitizedPrompt.substring(0, 80)}\nfunction generatedSolution() {\n  console.log("Generated by BigDaddyG");\n}`;
    }

    return `I understand your request: "${sanitizedPrompt}".\n\nThis is a fallback response generated locally because no AI backends were reachable. Please ensure Ollama or your configured API key is available for full fidelity responses.`;
  }
}

module.exports = OrchestraRemote;
